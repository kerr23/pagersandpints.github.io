{"categories":[],"posts":[{"content":"Through the history of technology, configuration management has driven many design patterns. Empires have been built upon things such as Chef, Ansible, or Puppet all with the intent to remove barriers in managing configuration files for applications deployed in various places. One of the challenges with the traditional approach is the people who understand the configuration(the QA team, developers etc.) are usually different from the people who are managing the configs in Chef. This can create an awkward back and forth with changing application tunables or updating endpoints between different departments, This industry challenge has driven a lot of effort to reduce barriers between the development team, and the teams that manage/monitor production systems, and while this effort and conversation has been good it is by no means a solved problem.\nNow add Docker to all this and you're presented with even more teeth to gnaw. Because one of the selling points of Docker is it's binary integrity, there have been several approaches made to try and integrate configuration management inside containers. People have tried to Chef/Puppet inside a container at startup, or bake all configuration into an app at build time, or rebuild the container every time you deploy.\nNone of these really solve anything. Running chef/puppet inside a container is problematic, and doesn't remove the challenge with who manages the config data. If you put all the config data in the container at build-time you're creating a security vulnerability by comingling production and development config data. Rebuilding the containers removes the binary-integrity advantage that docker gives you.\nOne approach I've found to work well is to use Hashicorp Consul to provide a key/value store for your configuration. Because the k/v interface is straight forward, you can create a solution that allows the team owning the service to see and manage it's configs from your dev environments to wherever else it gets deployed.\nBy utilizing envconsul you can preload a process shell environment with variables named after consul keys. This means if your applications are designed to expect their configuration to come from the environment, you can create a workflow pipeline to store your configurations in an easy to access, human readible, and highly available consul cluster. I'm not going to cover setting up a consul cluster, or any of the steps you take in monitoring or backing up the data, I'm going to focus solely on how to pull configuration data for your application.\nSetup is fairly straight forward In this example we'll use a config file using erb syntax in a rails application, however this would work with any application that expects values to exist in the shell environment it's run in.\ntest_value: \u0026lt;%= ENV['RAILS_TEST']%\u0026gt;\nEnvconsul allows you to strip a pre-deteremined section off the start of the key name. This allows you to segregate your keys based on environments while keeping the configuration files the same. Decide on a key structure that best fits your needs. The approch I would recommend is creating a top-level key an sub keys for each of the envirnoments you deploy your code to. Ex: /environment/staging and /environment/production and then under that all your key's will exist. Creating the key for the example we have would look something like.\ncurl -X PUT consul:8500/v1/kv/environment/staging/rails_test --data 'foo' curl -X PUT consul:8500/v1/kv/environment/production/rails_test --data 'bar'\nThen in your Dockerfile when you start your application you want to start it via envconsul. Envconsul provides a series of flags so I recommend creating a startup.sh file that is your application entry point. For an example rails app, my startup file would look something like /usr/local/bin/envconsul -consul-addr=consul:8500 -once -sanitize -upcase -prefix environment/$DEPLOYED_ENVIRONMENT bundle exec puma -e $RAILS_ENV -b tcp://0.0.0.0:9292\n -consul-addr is pretty straight forward -once keeps envconsul from attempting to continually poll consul. That feature doesn't work doesn't work well with rails, so we're limiting it to poll once. YMMV with this funcationality. -sanitize This removes any non-standard values from the key and replace them with underscores -upcase By upcasing all variables you guarentee a case you're expecting. -prefix The portion of your consul k/v data to focus on and strip off. and the rest is the command to run, in this case we're launching puma via bundler  When I go to run my docker container, I simply pass the RAILS_ENV and DEPLOYED_ENVIRONMENT values in at run time.\ndocker run -e RAILS_ENV=production -e DEPLOYED_ENVIRONMENT=staging my_app\nI've created an example repo that creates a rails app and a locally running consul server to demonstrate this very thing.\n","id":0,"section":"posts","summary":"Through the history of technology, configuration management has driven many design patterns. Empires have been built upon things such as Chef, Ansible, or Puppet all with the intent to remove barriers in managing configuration files for applications deployed in various places. One of the challenges with the traditional approach is the people who understand the configuration(the QA team, developers etc.) are usually different from the people who are managing the configs in Chef.","tags":["docker","consul"],"title":"Containers, Configuration, and Consul","uri":"https://pagersandpints.com/2019/05/containers-consul-configuration/","year":"2019"},{"content":"I recently had the pleasure of Amazon telling me that they had to reboot all of my Postgres RDS instances to apply some security patches.\nWhen using RDS you generally expect that Amazon is going to do something like this and I was at least happy that they told me about it and gave me the option to trigger it on a specific maintenance window or else on my own time (up to a drop dead date where they'd just do it for me)\nOne thing that you can't really know is what the impact of the operation is going to be. You know it's a downtime, but for how long?\nMy production instances, are of course, Multi-AZ but all of my non-production instances are not.\nFortunately, my non-production instances and my production instances both needed to get rebooted, so I could do some up-front testing on the timing.\nWhat I found was that the process takes about 10 to 15 minutes and, in this particular case, it was not impacted by database size.\nAlthough it is impacted by the number of instances you're rebooting at the same time. It seems Amazon queues the instances up so that some instances take longer than others.\nThe pre-reboot security patches took about 5 minutes to load during this time the database was up.\nThis was followed by a shutdown / reboot during which the database was unavailable.\nAfter the reboot which took less than a minute the database was immediately available while the system did post processing.\nAfter that a backup is performed which doesn't impact the system.\nSo total downtime was about a minute, but I scheduled 10 minutes just to be safe.\nFor the Multi-AZ instances the same process is followed but the shutdown / reboot is accompanied by an AZ failover which takes place nearly instantly. This is pretty cool as long as your applications are robust enough to re-connect. (Mine were not, so they required a restart) I timed the reboot to go with a deploy so no additional downtime was required.\nIn the end it was fairly painless, if you don't trust your applications ability to reconnect it's good to baby sit them. Otherwise kicking it off during a maintenance window and not worrying about it is certainly doable.\n","id":1,"section":"posts","summary":"I recently had the pleasure of Amazon telling me that they had to reboot all of my Postgres RDS instances to apply some security patches.\nWhen using RDS you generally expect that Amazon is going to do something like this and I was at least happy that they told me about it and gave me the option to trigger it on a specific maintenance window or else on my own time (up to a drop dead date where they'd just do it for me)","tags":["aws","postgresql"],"title":"Amazon Maintenance and your RDS Instances","uri":"https://pagersandpints.com/2016/02/amazon-maintenance-and-your-rds-instances/","year":"2016"},{"content":"Two simple gems to help tune your Postgres databases.\nI'm a big fan of PgTune. I think that in many cases you can run PgTune and set-it-and-forget-it for your Postgres parameters. I like it so much that I often wish I had access to it in my code - especially when working with Puppet to provision new databases servers.\nWhen I started looking into RDS Postgres a while back I realized that the default configuration for those instances was lacking and I really wished I could run PgTune on the RDS instances.\nIt was to solve those problems above that these two projects formed.\n RDSTune will create a MrTuner-ized RDS Parameter Group  MrTuner is a Ruby gem that follows in the sprit of PgTune if not directly in it's footsteps.  Both will run from the command line but, more importantly, they can be `required` by your ruby projects to allow you to access these values programmatically. Both Gems are available on rubygems and source, examples, configuration and docks available at their respective bitbucket pages.\nRDSTune - https://bitbucket.org/davidkerr/rdstune\nMrTuner - https://bitbucket.org/davidkerr/mrtuner\nFeedback and Pull requests very welcome!\n","id":2,"section":"posts","summary":"Two simple gems to help tune your Postgres databases.\nI'm a big fan of PgTune. I think that in many cases you can run PgTune and set-it-and-forget-it for your Postgres parameters. I like it so much that I often wish I had access to it in my code - especially when working with Puppet to provision new databases servers.\nWhen I started looking into RDS Postgres a while back I realized that the default configuration for those instances was lacking and I really wished I could run PgTune on the RDS instances.","tags":["aws","postgresql"],"title":"Introducing MrTuner and RDSTune","uri":"https://pagersandpints.com/2015/01/introducing-mrtuner-and-rdstune/","year":"2015"},{"content":"I was working with a friend of mine on his startup Down For Whatever and he wanted to use Docker.\nI created a docker farm for him and we're using Centurion for deployments, we signed up for a docker hub account to store images and started pushing code.\nA few days later he emailed me saying that he wanted to switch to capistrano for code deployments instead of building a docker image each time because pushing the image to docker hub took too damn long. (upwards of 10 minutes for him at it's worst)\nThat felt wrong, and kind of dirty. To me, docker is about creating a bundle of code that you can deploy anywhere. Not about creating a bundle of infrastructure that then you can then deploy into.\nIt was also surprising because I had started off with a Dockerfile based on Brian Morearty's blog post about skipping the bundle install each time you build a docker image. So I didn't think I had a lot of optimization left available to me.\nBut once we got into Golfing around with the Dockerfile and the .dockerignore file we found massive improvements to be had.\nThe .dockerignore file One of the issues that we found out right away was that he had 500M of log files in his app's log directory. So we added log/* to the .dockerignore. We were also picking up the .git directory, the db/*.sqlite3 databases, etc.\nOnce we removed those we dropped around 600M off of the size of our docker image, and that helped quite a bit.\nHere's what we ended up with in the .dockerignore\n.git config/centurion log/* db/*.sqlite3 db/*.sqlite3-journal .bundle tags vendor/cache/* tmp/* *.sh Procfile* test spec The Dockerfile Next we looked into the Dockerfile itself. Here's what we started with.\nFROMdfw1/base:latest  USERroot RUN mkdir -p /opt/app/dfw WORKDIR/opt/app/dfw ADD Gemfile /opt/app/dfw/Gemfile ADD Gemfile.lock /opt/app/dfw/Gemfile.lock RUN chown -R app:app /opt/app/dfw USERapp RUN jruby -S bundle install --without development test --no-color --path /opt/app/dfw ADD . /opt/app/dfw USERroot RUN chown -R app:app /opt/app/dfw ADD ./container/dfw-supervisor.conf /etc/supervisor.d/dfw.conf ADD ./container/dfw-nginx.conf /etc/nginx/sites-enabled/dfw.conf USERapp RUN EXECJS\\_RUNTIME=\u0026#39;Node\u0026#39; JRUBY\\_OPTS=\u0026#34;-X-C\u0026#34; RAILS_ENV=production jruby -S bundle exec rake assets:precompile USERroot Yeah, that kind of sucks. So we started some Dockerfile golf.\n( The base image that I call is basically \u0026ldquo;Install a bunch of stuff and then run supervisord as my CMD\u0026rdquo; )\nOptimization Goals I kept a couple of goals in mind while going through the dockerfile.\n Create as few layers as possible I actually had this drilled in me from the early days of Docker back when AUFS had a 42 layer limit so I was already grouping my \u0026lsquo;RUN\u0026rsquo; commands and trying to be as sparse as possible elsewhere. There should only be one big layer that isn't cached and that's the layer with the code in it.  We found that with the above dockefile we were pushing a 70 MB layer and then a 100 MB layer and 15 MB layer. That was too many layers and too large.   What We Learned Avoid USER switching (if you can)\nThe first thing that I didn't like was all of the USER switching. Each switch is a layer that needs to be pushed, and though it's small pushing still takes a few seconds each for the transfer and for the layer verification.\nIn my new dockerfile all of the RUN commands run as root. The application itself runs as the app user thanks to supervisord.\nYou just need to know where to fix the permissions after the fact.\nSequence Matters!\nLook at all the work i'm doing AFTER the line: ADD . /opt/app/dfw Nothing after that line will ever be cached (because we're building a docker image because we've changed code) even though a lot of it will rarely change.\nMove the static ADDs like supervisord config's to the top of the Dockerfile before the un-cacheable add.\nAfter those two changes we're looking better\nFROMdfw1/base:latest  RUN mkdir -p /opt/app/dfw WORKDIR/opt/app/dfw ADD ./container/dfw-supervisor.conf /etc/supervisor.d/dfw.conf ADD ./container/dfw-nginx.conf /etc/nginx/sites-enabled/dfw.conf ADD Gemfile /opt/app/dfw/Gemfile ADD Gemfile.lock /opt/app/dfw/Gemfile.lock RUN chown -R app:app /opt/app/dfw RUN jruby -S bundle install --without development test --no-color --path /opt/app/dfw ADD . /opt/app/dfw RUN chown -R app:app /opt/app/dfw RUN EXECJS\\_RUNTIME=\u0026#39;Node\u0026#39; JRUBY\\_OPTS=\u0026#34;-X-C\u0026#34; RAILS_ENV=production jruby -S bundle exec rake assets:precompile But there are a few things we can still do.\nTry to DRY up the Dockerfile My \u0026ldquo;dfw1/base\u0026rdquo; image has a massive \u0026lsquo;RUN\u0026rsquo; in it that's joined by a bunch of \u0026amp;\u0026amp;\u0026lsquo;s I have some things in this file that could be moved up into that RUN so that it's just 1 layer.\nSpecifically my mkdir -p /opt/app/dfw.\nSure, not all of the apps that use that base image will use /opt/app/dfw but it doesn't hurt those apps for it to be there as an empty directory.\nAlso, since all of my apps will live in /opt/app/ and be run as \u0026lsquo;app\u0026rsquo; I can take out the first chown -R app:app /opt/app/dfw and have it in the \u0026lsquo;base\u0026rsquo; image as just chown -R app:app /opt/app (that also helps squash the dockerfile for other apps i build off that image)\nBeware the chown!\nThis one was a huge /facepalm for me.\nLook at what i do:\nADD . /opt/app/dfw RUN chown -R app:app /opt/app/dfw I HAVE to chown the dfw directory to app:app because Docker ADDs create files with UID 0. So app can't write to it's logs or create any cache files, etc.\nBut think about what we've done. Layer 1: ADD ~72M of source Layer 2: touch every file in that 72M of source to update it's user info.\nBasically I now have two 72M layers that need to be pushed!\nThis one is a puzzler to fix. I solved it by changing my CMD strategy. The only CMD in my images was CMD \\[\u0026quot;supervisord\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;/etc/supervisord.conf\u0026quot;\\]\nI switched that to call a shell script:\nchown -R app:app /opt/app supervisord -c /etc/supervisord.conf Now I could take all the chown's out of my images and we're not duplicating layers unnecessarily.\nIt also opened up quite a few optimization paths for my general workflow to add other things to that shell script instead of doing it in the layers.\nNot everything needs to be done in the Dockerfile\nAt this point we're really well optimized. However there's one thing that still took quite a long time (as it usually does) and that's precompiling the assets for Rails.\nMy friend said, \u0026ldquo;Can't we just do that outside of docker?\u0026rdquo; I'm not a rails expert by any means, but he is and if he thought we could do it I'd give it a try.\nWe were already using a build script to create / push our docker images. It's effectively.:\ndocker build -t dfw1/dfw:latest . \u0026amp;\u0026amp; docker push dfw1/dfw:latest\nWe're developing on macs using boot2docker, which means that really all of the \u0026lsquo;docker build\u0026rsquo; work is being done inside a virtualbox vm, which isn't the fastest way.\nAlso, in the container we only have jruby, which takes a bit of time to bootstrap itself.\nRunning the rake ahead of time cuts down the build time pretty significantly.\nSummary In the end we knocked about 9 minutes or more off the build/push of our dockerfiles and my friend was no longer calling for moving to capistrano. Our Dockerfile ended up being really nice and concise\nFROMdfw1/base:latest ADD Gemfile /opt/app/dfw/Gemfile ADD Gemfile.lock /opt/app/dfw/Gemfile.lock RUN RAILS_ENV=production /opt/jruby/bin/jruby -S bundle install --without development test --no-color --path /opt/app/dfw ADD . /opt/app/dfw It's still not perfect. The push phase still takes way to long, and the \u0026lsquo;already pushed\u0026rsquo; images take a couple seconds each to determine that they've already been pushed.\nWe've speculated that if the push phase could run in parallel, or just faster we could shave 20 seconds or more off the build. But as it is, we're content with what we have.\n","id":3,"section":"posts","summary":"I was working with a friend of mine on his startup Down For Whatever and he wanted to use Docker.\nI created a docker farm for him and we're using Centurion for deployments, we signed up for a docker hub account to store images and started pushing code.\nA few days later he emailed me saying that he wanted to switch to capistrano for code deployments instead of building a docker image each time because pushing the image to docker hub took too damn long.","tags":["docker"],"title":"Dockerfile Golf (or optimizing the Docker build process)","uri":"https://pagersandpints.com/2014/08/dockerfile-golf-or-optimizing-the-docker-build-process/","year":"2014"},{"content":"It seems like the folks at Amazon set some strange defaults for their RDS Postgres instances and they make it pretty darn difficult to allow for dynamically sized instance.\nYou tune your Postgres RDS instance via Parameter Groups. In the parameter group configuration is all of the normal PG tuning parameters from your postgresql.conf.\nThey provide you with a variable: {DBInstanceClassMemory} which returns the memory in bytes available to the instance, and you can use that in some limited ways to dynamically set parameters based on the instance type you chose for your RDS database. There may be more of these variables but I wasn't able to find them.\nOne commenter pointed out that DBInstanceClassMemory is possibly not the entire memory of the machine. So for example: DBInstanceClassMemory on an m1.xlarge would not be 16106127360 (16GB) but instead they lower it to take into account the memory allocated to the OS. I hope that this will be changed in the futures since most postgres tuning guides are based on total system memory and not mem - arbitrary OS usage.\nIt'd be easy to just hop in and hardcode you're values, but that's not really very fun now is it? And besides we want to be able to grow our instances or spin up smaller / larger read replicas w/o defining a Parameter Group for each size we might want. So I'm endeavoring to make my parameter group as dynamic as possible.\nI spun up an m1.xlarge RDS instance and did some playing around.\nBefore we get stared: one thing to note is that you don't want any spaces between the {}\u0026lsquo;s when doing calculations, otherwise AWS will respond with a format error. It also seems to have problems with multiple divisions or multiplications. Some areas they take, often they don't.\nBelow is a list of configuration parameters that I usually set in all my databases (which I base on PgTune). Naturally you'd want to set these based on your needs, but I find that PgTune gets you 90% there most of the time.\nEdit: As some commenters have pointed out I got a few of the calculations wrong by not taking into account that RDS was using kb for some parameters, or other reasons (I actually suspect that amazon made a change to how they're calculating these parameters). I've updated the values below with the correct / current information.\nmax_connections  We all know that max_connections is best expressed at a function of # of CPUs, however Amazon went with this formula: {DBInstanceClassMemory/12582880} - which on the m1.large would leave you with 604 connections\u0026hellip;. Unless Amazon added a connection pooler to postgres I don't see how that would possibly work.\nFolks migrating from MySQL or Oracle could easily get tripped up by not paying proper attention that that parameter.\nI usually keep max connections to around 200 just because I usually deal with Ruby/Java where I need to oversubscribe a bit.\n shared_buffers  The default here is: {DBInstanceClassMemory/32768} which is correct. Originally it didn't seem to calculate correctly so I had recommended a change.\n checkpoint_segments The default # of checkpoint segments is 16 which is a pretty good base, however if you're a web application (which you probably are if you're using RDS) PgTune recommends that you set it to 8.\n work_mem work_mem is a little tricky. I, and pgtune, usually base it on max_connections to calculate this value. However I haven't managed to find a way to reference a database parameter from within the RDS parameter section. For example: {max_connections} doesn't work.\nSo you'll need to hard code something like your connections in there.\nSo work_mem: {DBInstanceClassMemory/204800} # (200 (max_connections) * 1024 = 204800)\nOn an m1.xlarge that would give you ~74MB which is what pgtune would give you on the same instance.\n maintenance_work_mem For this parameter RDS just uses the postgres default which is 16MB so creating indexes on large tables would end up taking a few years.\nI try to keep this one around 1GB if the machine has enough memory. A nice rule of thumb though would be System Memory (in bytes) / 16 (which is what pgtune generates).\nWe need to take into account the fact that RDS deals in kb for this parameter so: {DBInstanceClassMemory/16384} (16*1024)\nThis gives us 924MB on an m1.xlarge\n effective_cache_size Effective cache size is generally tricky in AWS since even with provisioned IOPS you're getting slightly better than thumb-drive speed out of your disks. So you may benefit from a smaller effective_cache_size.\nAs commenter Alexandre Russel pointed out RDSs unit of measure for effective_cache_size is in 8kB blocks\nThe effective_cache_size defaults associated with RDS are {DBInstanceClassMemory/16384}\nSo DBInstanceClassMemory / 2 * 8k or, in other words, 50%.\nTo get it closer to the 75% we would normally use we can try (75/(100*8192))\n{DBInstanceClassMemory*75/819200} which brings us up to a little over 10GB for an m1.xlarge.\n wal_buffers Uses the postgres default of 2048 bytes. I usually do 512 * checkpoint segments for this one so you may need to just hardcode it in at 4096\n checkpoint_completion_target Again no good way to dynamically set this one, fortunately it's pretty fixed for a web workload at 0.7\n","id":4,"section":"posts","summary":"It seems like the folks at Amazon set some strange defaults for their RDS Postgres instances and they make it pretty darn difficult to allow for dynamically sized instance.\nYou tune your Postgres RDS instance via Parameter Groups. In the parameter group configuration is all of the normal PG tuning parameters from your postgresql.conf.\nThey provide you with a variable: {DBInstanceClassMemory} which returns the memory in bytes available to the instance, and you can use that in some limited ways to dynamically set parameters based on the instance type you chose for your RDS database.","tags":["aws","postgresql"],"title":"Tune your Postgres RDS instance via parameter groups","uri":"https://pagersandpints.com/2013/11/tune-your-postgres-rds-instance-via-parameter-groups/","year":"2013"},{"content":"I recently submitted a (very) small patch for PostgreSQL to fix a problem we were having with the backup label not getting fsynced to disk. (which is a problem if you're using something like AWS or SAN snapshots.)\n I kindly got a mention in the release notes for it, pretty cool! (thanks guys!)\n","id":5,"section":"posts","summary":"I recently submitted a (very) small patch for PostgreSQL to fix a problem we were having with the backup label not getting fsynced to disk. (which is a problem if you're using something like AWS or SAN snapshots.)\n I kindly got a mention in the release notes for it, pretty cool! (thanks guys!)","tags":["postgresql"],"title":"Mention in the Postgres release notes","uri":"https://pagersandpints.com/2012/10/mention-in-the-postgres-release-notes/","year":"2012"},{"content":"Update 10/15/2012\nWith the release of Postgres 9.1.6 I believe that what we were running into was the critical bug that's fixed in that release regarding WAL replay.\nSo Mystery Solved!\nI've recently come across a problem with Streaming Replication where on failover the system generates a PANIC: WAL contains references to invalid pages error message.\nAug 9 14:06:22 db01 postgres[11005]: [8-1] user=,db=,host= LOG: trigger file found: /db/9.1/data/failover Aug 9 14:06:22 db01 postgres[11012]: [3-1] user=,db=,host= FATAL: terminating walreceiver process due to administrator command Aug 9 14:06:23 db01 postgres[11005]: [9-1] user=,db=,host= LOG: invalid record length at 398/1D0002F0 Aug 9 14:06:23 db01 postgres[11005]: [10-1] user=,db=,host= LOG: redo done at 398/1D000298 Aug 9 14:06:23 db01 postgres[11005]: [11-1] user=,db=,host= LOG: last completed transaction was at log time 2012-08-09 09:16:08.497124+00 Aug 9 14:06:23 db01 postgres[11005]: [12-1] user=,db=,host= LOG: selected new timeline ID: 10 Aug 9 14:06:23 db01 postgres[11005]: [13-1] user=,db=,host= LOG: archive recovery complete Aug 9 14:06:23 db01 postgres[11005]: [14-1] user=,db=,host= WARNING: page 1563020 of relation base/16436/47773 was uninitialized Aug 9 14:06:23 db01 postgres[11005]: [15-1] user=,db=,host= WARNING: page 1563023 of relation base/16436/47773 was uninitialized Aug 9 14:06:23 db01 postgres[11005]: [16-1] user=,db=,host= WARNING: page 1563021 of relation base/16436/47773 was uninitialized Aug 9 14:06:23 db01 postgres[11005]: [17-1] user=,db=,host= WARNING: page 1563022 of relation base/16436/47773 was uninitialized Aug 9 14:06:23 db01 postgres[11005]: [18-1] user=,db=,host= WARNING: page 792619 of relation base/16436/47776 was uninitialized Aug 9 14:06:23 db01 postgres[11005]: [19-1] user=,db=,host= PANIC: WAL contains references to invalid pages Aug 9 14:06:49 db01 postgres[11003]: [2-1] user=,db=,host= LOG: startup process (PID 11005) was terminated by signal 6: Aborted Aug 9 14:06:49 db01 postgres[11003]: [3-1] user=,db=,host= LOG: terminating any other active server processes I believe that this has something to do with the methodology that I use to resync my mirrors after a failover. The environments that I've seen this on have gone through a lot of failover testing. The scenario looks something like:\nA[Master] =\u0026gt; B[Standby] ...Failover... B[Master] A[Down] rsync A from B B[Master] =\u0026gt; A[Standby] ...Failback... A[Master] B[Down] resync B from A etc. The process I use to sync, and resync doesn't change.\n#!/bin/ksh SOURCE=$1 if \\[ -z \u0026quot;$SOURCE\u0026quot; \\]; then echo \u0026quot;Usage: $0 \u0026quot; exit 1 fi . /etc/sysconfig/pgsql/postgresql-9.1 export PGDATA export PGPORT rm -f $PGDATA/failover pg_ctl stop -D $PGDATA -m immediate psql -h $SOURCE -p $PGPORT \u0026lt;\u0026lt;EOD checkpoint; select pg\\_start\\_backup('mirror'); EOD rsync -avv --delete-delay \\ --exclude postgresql.conf \\ --exclude pg_hba.conf \\ --exclude server.conf \\ --exclude archive.conf \\ --exclude recovery.conf \\ --exclude recovery.done \\ --exclude pg_ident.conf \\ --exclude failover \\ --exclude pg_xlog \\ --exclude postmaster.pid \\ $SOURCE:$PGDATA/ $PGDATA psql -h $SOURCE -p $PGPORT -c \u0026quot;select pg\\_stop\\_backup()\u0026quot; cp $PGDATA/recovery.done $PGDATA/recovery.conf pg_ctl start -D $PGDATA I don't think anything inherently wrong with this method, so I could be coming across a linux bug or a Postgres bug.\nSome investigation with the fine fellows at 2nd Quadrant has shown that the failed pages are all in index relations.\nOur next step in debugging the process will be to start the server with DEBUG3 and see if we can get more info. Unfortunately I've blown away that mirror, so I have to wait for it to happen again.\nStay tuned.\n","id":6,"section":"posts","summary":"Update 10/15/2012\nWith the release of Postgres 9.1.6 I believe that what we were running into was the critical bug that's fixed in that release regarding WAL replay.\nSo Mystery Solved!\nI've recently come across a problem with Streaming Replication where on failover the system generates a PANIC: WAL contains references to invalid pages error message.\nAug 9 14:06:22 db01 postgres[11005]: [8-1] user=,db=,host= LOG: trigger file found: /db/9.1/data/failover Aug 9 14:06:22 db01 postgres[11012]: [3-1] user=,db=,host= FATAL: terminating walreceiver process due to administrator command Aug 9 14:06:23 db01 postgres[11005]: [9-1] user=,db=,host= LOG: invalid record length at 398/1D0002F0 Aug 9 14:06:23 db01 postgres[11005]: [10-1] user=,db=,host= LOG: redo done at 398/1D000298 Aug 9 14:06:23 db01 postgres[11005]: [11-1] user=,db=,host= LOG: last completed transaction was at log time 2012-08-09 09:16:08.","tags":["postgresql"],"title":"Streaming Replication, syncing mirrors and PANIC: WAL contains references to invalid pages","uri":"https://pagersandpints.com/2012/08/streaming-replication-syncing-mirrors-and-panic-wal-contains-references-to-invalid-pages/","year":"2012"},{"content":"I run Postgres on a fairly large linux server with 256G of ram.\nDuring high load, I found that the I/O of the $PGDATA volume was spiking to 100% making the database slow down to a crawl for seconds at a time, despite having a fairly fast I/O subsystem.\nThis is what the spike looked like from an iostat output:\nDate r/s w/s rsec/s wsec/s await svctm %util\n[\u0026hellip;]\n07/10/12 00:35:36 0 69.8 0 2233.6 0.63 0.07 0.46\n07/10/12 00:35:41 1.2 810 99.2 22200 4.13 0.05 4.02\n07/10/12 00:35:46 0 111.6 0 5422.4 1.82 0.08 0.9\n07/10/12 00:35:51 0 299.2 0 5670.4 1.27 0.04 1.24\n07/10/12 00:35:56 0.8 176.6 41.6 3654.4 2.16 0.07 1.32\n07/10/12 00:36:01 0 364.8 0 6670.4 1.1 0.04 1.62\n07/10/12 00:36:06 0.8 334.6 12.8 5953.6 1.18 0.05 1.64\n07/10/12 00:36:11 0 118.6 0 6948.8 1.82 0.07 0.82\n07/10/12 00:36:16 0 8274.6 0 148764.8 10.55 0.07 61.18\n07/10/12 00:36:21 0.2 8577.4 3.2 161806.4 16.68 0.12 99.62\n07/10/12 00:36:26 0.8 9244.6 12.8 167841.6 15.01 0.11 99.82\n07/10/12 00:36:31 0.8 9434.2 44.8 208156.8 16.22 0.11 99.7\n07/10/12 00:36:36 0 9582.8 0 202508.8 14.84 0.1 99.72\n07/10/12 00:36:41 0 9830.2 0 175326.4 14.42 0.1 99.5\n07/10/12 00:36:46 0 8208.6 0 149372.8 17.82 0.12 99.64\n07/10/12 00:36:51 3 1438.4 102.4 26748.8 8.49 0.12 18\n07/10/12 00:36:56 0.6 2004.6 9.6 27400 1.25 0.03 5.74\n07/10/12 00:37:01 0.6 1723 9.6 23758.4 1.85 0.03 5.08\n07/10/12 00:37:06 0.4 181.2 35.2 2928 1.49 0.06 1.06\nThe Linux I/O subsystem can get overwhelmed with dirty buffers when using a system with high memory + high volume.\nIf you consider that on a 256GB system - the defaults for RHEL 6.2:\nvm.dirty_ratio = 10 == 26Gb\nvm.dirty_background_ratio = 5 == 13Gb\nYou can see that you can have quite a bit of data to flush out when the fsync() is called as part of a checkpoint\nTo alleviate the problem there are 2 new kernel parameters:\nvm.dirty_bytes and vm.dirty_background_bytes\nThese settings allow you to specify a much smaller value for your filesystem buffer.\nThese settings take some tweaking to get right - you may consider the size of your RAID cache or your general I/O throughput.\nIn my case I found that the following settings worked well with my disk subsystem.\nvm.dirty_background_bytes = 33554432 # 32MB\nvm.dirty_bytes = 268435456 #256MB\nYou can see the difference in the following chart, the blue line is the default and the red line is with the new VM settings.\nYou can see that you're using slightly more I/O with the new settings but the spike doesn't end up happening.\n\nThanks to Maxim, Jeff and Andres in this thread for pointing me in te right direction And Greg who tried to explain it to me but I was too block headed at that time to get it.\n","id":7,"section":"posts","summary":"I run Postgres on a fairly large linux server with 256G of ram.\nDuring high load, I found that the I/O of the $PGDATA volume was spiking to 100% making the database slow down to a crawl for seconds at a time, despite having a fairly fast I/O subsystem.\nThis is what the spike looked like from an iostat output:\nDate r/s w/s rsec/s wsec/s await svctm %util\n[\u0026hellip;]\n07/10/12 00:35:36 0 69.","tags":["postgresql"],"title":"I/O Spikes during Checkpoint","uri":"https://pagersandpints.com/2012/07/i-o-spikes-during-checkpoint/","year":"2012"},{"content":"I'm toying around with pg_service.conf. It wasn't obvious - but I was able to determine that if you add pg_service.conf to /etc/sysconfig/pgsql/ it will get picked up globally.\n","id":8,"section":"posts","summary":"I'm toying around with pg_service.conf. It wasn't obvious - but I was able to determine that if you add pg_service.conf to /etc/sysconfig/pgsql/ it will get picked up globally.","tags":["linux","postgresql"],"title":"pg_service.conf in redhat","uri":"https://pagersandpints.com/2012/06/pg_service-conf-in-redhat/","year":"2012"},{"content":"psql the postgresql command line client is usually compiled with libreadline support. As a result of this you can use some neat functionality such as reverse search by hitting ctl-r within the client.\nreport=# hit CTL-r\n(reverse-i-search)`': start typing\n(reverse-i-search)`sel': select * from report.audit where\n","id":9,"section":"posts","summary":"psql the postgresql command line client is usually compiled with libreadline support. As a result of this you can use some neat functionality such as reverse search by hitting ctl-r within the client.\nreport=# hit CTL-r\n(reverse-i-search)`': start typing\n(reverse-i-search)`sel': select * from report.audit where","tags":["postgresql"],"title":"psql - reverse search history","uri":"https://pagersandpints.com/2012/06/psql-reverse-search-history/","year":"2012"},{"content":"oid2name is a nifty little program comes with postgresql an it allows you to take those obscure relfilenode named datafiles at the OS level and map them to database objects.\nThe only downside to this little tool is that it doesn't exactly work when your database is down since it uses pg_* to figure out this info. So if you're in a situation where you're restoring the database and find out that you're missing a file from the backup - you may want to know what table is lost.\nIf you thought ahead and have a manifest.txt in your backups you'll be able to see that the following maps to pgbench_accounts\n-rw------- 1 postgres postgres 13434880 Jun 7 04:44 /db/9.1/data/base/1952441/1952454\nIf you use the -ix flag you get schema, OID and tablespace. If you also add the -S flag you get all the system tables too which is very important in a recovery situation.\n/usr/pgsql-9.1/bin/oid2name -ix -d pgbench From database \u0026ldquo;pgbench\u0026rdquo;:\n Filenode Table Name Oid Schema Tablespace -------------------------------------------------------------- 1952454 pgbench_accounts 1952448 public pg_default 1952459 pgbench_accounts_pkey 1952459 public pg_default 1952442 pgbench_branches 1952442 public pg_default 1952455 pgbench_branches_pkey 1952455 public pg_default 1952451 pgbench_history 1952451 public pg_default 1952445 pgbench_tellers 1952445 public pg_default 1952457 pgbench_tellers_pkey 1952457 public pg_default ","id":10,"section":"posts","summary":"oid2name is a nifty little program comes with postgresql an it allows you to take those obscure relfilenode named datafiles at the OS level and map them to database objects.\nThe only downside to this little tool is that it doesn't exactly work when your database is down since it uses pg_* to figure out this info. So if you're in a situation where you're restoring the database and find out that you're missing a file from the backup - you may want to know what table is lost.","tags":["PostgreSQL"],"title":"Create a backup manifest in PostgreSQL with oid2name","uri":"https://pagersandpints.com/2012/06/create-a-backup-manifest-in-postgresql-with-oid2name/","year":"2012"},{"content":"Here is a little perl script that I use to give me a list of tables in a schema, a rowcount and the pg_relation_size of a table. The usage is just:\npg_rowcount ex:./pg\\_rowcount.pl david.kerr pg\\_catalog\nThe output looks like this:\nTable Rows Size -------------------------------------------------------------- pg_statistic 1853 1672 kB pg_type 910 168 kB pg_attribute 9569 1488 kB pg_class 1371 416 kB pg_authid 52 16 kB pg_index 766 112 kB pg_operator 705 104 kB pg_database 44 16 kB You can download the script here\n","id":11,"section":"posts","summary":"Here is a little perl script that I use to give me a list of tables in a schema, a rowcount and the pg_relation_size of a table. The usage is just:\npg_rowcount ex:./pg\\_rowcount.pl david.kerr pg\\_catalog\nThe output looks like this:\nTable Rows Size -------------------------------------------------------------- pg_statistic 1853 1672 kB pg_type 910 168 kB pg_attribute 9569 1488 kB pg_class 1371 416 kB pg_authid 52 16 kB pg_index 766 112 kB pg_operator 705 104 kB pg_database 44 16 kB You can download the script here","tags":["postgresql"],"title":"A script to see row counts and table size in PostgreSQL","uri":"https://pagersandpints.com/2012/05/a-script-to-see-row-counts-and-table-size-in-postgresql/","year":"2012"},{"content":"Every once in a while CUPS printing just stalls, no reason that I can tell. (I blame the network, naturally) but what ends up happening is that the print jobs get backed up and you need to clear out the queue. The fastest way to do this is to do:\nrm /var/spool/cups/* /etc/init.d/cups restart ","id":12,"section":"posts","summary":"Every once in a while CUPS printing just stalls, no reason that I can tell. (I blame the network, naturally) but what ends up happening is that the print jobs get backed up and you need to clear out the queue. The fastest way to do this is to do:\nrm /var/spool/cups/* /etc/init.d/cups restart ","tags":["linux"],"title":"How To Fix Stalled CUPS Printing","uri":"https://pagersandpints.com/2012/05/how-to-fix-stalled-cups-printing/","year":"2012"},{"content":"I use a lot of \u0026ldquo;serial\u0026rdquo; data type fields, sometimes the nextval of my sequence doesn't reflect the max() of my table. Since I do a lot of moving data around these can get messed up. The basic command that you want to use to reset a sequence is:\nselect setval(\u0026#39;sequence_name\u0026#39;, value); However, the cool thing about Postgres is that can be a query too so I use:\nselect setval(\u0026#39;sequence_name\u0026#39;, select max(column_name) from tablename); So I wrote a little perl script to look through all of the columns that have a default like \u0026ldquo;nextval\u0026rdquo; (which if you look in the database is how a serial column actually looks) and sets the sequence to the max of the value in that table. You can get it here\nreset_requences.p -d [-s ] [-u ] [-p ] If you specify a database it will reset all of the columns in the database If you specify a schema it will reset all of the columns within a specific schema. If you specify a user it will reset all of the columns for a specific user.\n","id":13,"section":"posts","summary":"I use a lot of \u0026ldquo;serial\u0026rdquo; data type fields, sometimes the nextval of my sequence doesn't reflect the max() of my table. Since I do a lot of moving data around these can get messed up. The basic command that you want to use to reset a sequence is:\nselect setval(\u0026#39;sequence_name\u0026#39;, value); However, the cool thing about Postgres is that can be a query too so I use:\nselect setval(\u0026#39;sequence_name\u0026#39;, select max(column_name) from tablename); So I wrote a little perl script to look through all of the columns that have a default like \u0026ldquo;nextval\u0026rdquo; (which if you look in the database is how a serial column actually looks) and sets the sequence to the max of the value in that table.","tags":["postgresql"],"title":"How To Reset Sequences in PostgreSQL","uri":"https://pagersandpints.com/2012/05/how-to-reset-sequences-in-postgresql/","year":"2012"},{"content":"If you want to dummy up a bunch of data in PostgreSQL you can use this neat little trick\ncreate table test (a int); insert into test (select generate_series(0,999,1)); INSERT 0 1000; ","id":14,"section":"posts","summary":"If you want to dummy up a bunch of data in PostgreSQL you can use this neat little trick\ncreate table test (a int); insert into test (select generate_series(0,999,1)); INSERT 0 1000; ","tags":["postgresql"],"title":"Using generate_series() in PostgreSQL","uri":"https://pagersandpints.com/2012/05/using-generate_series-in-postgresql/","year":"2012"},{"content":"PostgreSQL provides the ability to return a value on insert. so for example:\ncreate table foo (id serial, name text); insert into foo(\u0026#39;dave\u0026#39;) returning id; Will return the auto-generated value for \u0026ldquo;id\u0026rdquo; assigned to \u0026lsquo;dave\u0026rsquo;\nYou can leverage this from within Perl by doing something like this:\nmy $foo_insert = \u0026#34;insert into foo(?) returning id\u0026#34; my $foo_cur = $dbh-\u0026gt;prepare($foo_insert); $foo_cur-\u0026gt;execute(\u0026#39;Dave\u0026#39;); my $foo_rec = $foo_cur-\u0026gt;fetchrow_hashref(); print $foo_rec{\u0026#34;id\u0026#34;}; ","id":15,"section":"posts","summary":"PostgreSQL provides the ability to return a value on insert. so for example:\ncreate table foo (id serial, name text); insert into foo(\u0026#39;dave\u0026#39;) returning id; Will return the auto-generated value for \u0026ldquo;id\u0026rdquo; assigned to \u0026lsquo;dave\u0026rsquo;\nYou can leverage this from within Perl by doing something like this:\nmy $foo_insert = \u0026#34;insert into foo(?) returning id\u0026#34; my $foo_cur = $dbh-\u0026gt;prepare($foo_insert); $foo_cur-\u0026gt;execute(\u0026#39;Dave\u0026#39;); my $foo_rec = $foo_cur-\u0026gt;fetchrow_hashref(); print $foo_rec{\u0026#34;id\u0026#34;}; ","tags":["postgresql"],"title":"Using insert .. returning with perl and PostgreSQL","uri":"https://pagersandpints.com/2012/05/using-insert-returning-with-perl-and-postgresql/","year":"2012"},{"content":"ppgfouine is a nice logfile analyzer for PostgreSQL written in php.\nWhen doing a trace on a very long running ETL process the logfile generated was ~11GB.\nThis left me running up against a 2GB barrier in php for fopen().\nIf you've got a 64bit machine and can recompile php with -D_FILE_OFFSET_BITS=64 then you're good to go. But in my case, I wasn't able to do either.\n The error i'd get is: PHP Fatal error: File is not readable. in /var/lib/pgsql/pgfouine-1.1/include/GenericLogReader.class.php on line 85\nThe simple solution was to use a named pipe since pgfouine expects a file and doesn't seem to be able to read from stdin.\nmknod /tmp/pg2 p\ncat /var/log/postgres \u0026gt; /tmp/pg2 | ./pgfouine.php -file /tmp/pg2 \u0026gt; bla.html\nAnother variation on this, for those of us using UTC and find that our postgres log is rotated mid-day is to do something like:\n(gzcat /var/log/postgres.xyz.gz \u0026amp;\u0026amp; cat /var/log/postgres) \u0026gt; /tmp/pg2 | ./pgfouine.php -file /tmp/pgt \u0026gt; bla.html\n","id":16,"section":"posts","summary":"ppgfouine is a nice logfile analyzer for PostgreSQL written in php.\nWhen doing a trace on a very long running ETL process the logfile generated was ~11GB.\nThis left me running up against a 2GB barrier in php for fopen().\nIf you've got a 64bit machine and can recompile php with -D_FILE_OFFSET_BITS=64 then you're good to go. But in my case, I wasn't able to do either.\n The error i'd get is: PHP Fatal error: File is not readable.","tags":["postgresql"],"title":"Parsing Large files with Pgfouine","uri":"https://pagersandpints.com/2012/05/parsing-large-files-with-pgfouine/","year":"2012"},{"content":"I frequently find the need to have single quoted output when i generate dynamic SQL.\nIt's always a pain to remember the exact number of ticks needed to get the quoted output.\nHere is a reminder on how to do it:\nselect \u0026#39;\u0026#39;\u0026#39;\u0026#39;||schemaname||\u0026#39;.\u0026#39;||tablename||\u0026#39;\u0026#39;\u0026#39;\u0026#39; from pg_tables i.e., four quotes when there's no text, or three quotes on the outside and one on the inside when there is text.\nselect \u0026#39;\u0026#39;\u0026#39;public.\u0026#39;||tablename||\u0026#39;\u0026#39;\u0026#39;;\u0026#39; from pg_tablesselect \u0026#39;select pg_size_pretty(pg_relation_size(\u0026#39;\u0026#39;\u0026#39;||tablename||\u0026#39;\u0026#39;\u0026#39;));\u0026#39; from pg_tables where schemaname = \u0026#39;bla\u0026#39;; ","id":17,"section":"posts","summary":"I frequently find the need to have single quoted output when i generate dynamic SQL.\nIt's always a pain to remember the exact number of ticks needed to get the quoted output.\nHere is a reminder on how to do it:\nselect \u0026#39;\u0026#39;\u0026#39;\u0026#39;||schemaname||\u0026#39;.\u0026#39;||tablename||\u0026#39;\u0026#39;\u0026#39;\u0026#39; from pg_tables i.e., four quotes when there's no text, or three quotes on the outside and one on the inside when there is text.\nselect \u0026#39;\u0026#39;\u0026#39;public.\u0026#39;||tablename||\u0026#39;\u0026#39;\u0026#39;;\u0026#39; from pg_tablesselect \u0026#39;select pg_size_pretty(pg_relation_size(\u0026#39;\u0026#39;\u0026#39;||tablename||\u0026#39;\u0026#39;\u0026#39;));\u0026#39; from pg_tables where schemaname = \u0026#39;bla\u0026#39;; ","tags":["postgresql"],"title":"How To Add Quotes to results in dynamic SQL","uri":"https://pagersandpints.com/2012/05/how-to-add-quotes-to-results-in-dynamic-sql/","year":"2012"}],"tags":[{"title":"aws","uri":"https://pagersandpints.com/tags/aws/"},{"title":"consul","uri":"https://pagersandpints.com/tags/consul/"},{"title":"docker","uri":"https://pagersandpints.com/tags/docker/"},{"title":"linux","uri":"https://pagersandpints.com/tags/linux/"},{"title":"postgresql","uri":"https://pagersandpints.com/tags/postgresql/"}]}